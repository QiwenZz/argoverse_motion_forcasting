{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8qF27Hueiw5"
      },
      "outputs": [],
      "source": [
        "# import libraries and packages\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os, os.path \n",
        "import pickle\n",
        "from glob import glob\n",
        "import itertools\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import *\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import normalize\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the master dataset\n",
        "from csv import reader\n",
        "from collections import defaultdict\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip /content/gdrive/MyDrive/train.zip"
      ],
      "metadata": {
        "id": "MD8mWxSuenLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Model\n",
        "- According to the Exploratory Data Analysis, there is only one `Agent` per csv file with a total of 50 timestamps\n",
        "- Our goal is the predict the latest 5 trajectories of the `Agent` using the previous 45 trajectories as training data\n",
        "- The input features are the x and y values for the `Agent` \n",
        "\n",
        "## Data Loader\n",
        "- Input features with size: batch size x 45 x 2\n",
        "- Output features with size: batch size x 5 x 2"
      ],
      "metadata": {
        "id": "6gqDIZj9ertZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ArgoverseDataset(Dataset):\n",
        "    \"\"\"Dataset class for Argoverse\"\"\"\n",
        "    def __init__(self, data_path: str, transform=None):\n",
        "        super(ArgoverseDataset, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "\n",
        "        self.csv_list = glob(os.path.join(self.data_path, '*'))\n",
        "        self.csv_list.sort()\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.csv_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        csv_path = self.csv_list[idx]\n",
        "        with open(csv_path, 'rb') as f:\n",
        "            data = pd.read_csv(f)\n",
        "            \n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "# intialize a dataset\n",
        "train_data  = ArgoverseDataset(data_path=\"/content/train/data\")\n",
        "#test_data  = ArgoverseDataset(data_path=\"/content/new_val_in/new_val_in/\")"
      ],
      "metadata": {
        "id": "-CXHhT8Keo1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_sz = 10\n",
        "def my_collate(batch):\n",
        "    # extract 50 timestamps for X values and Y values for each scene in batch\n",
        "    xs = [scene.loc[scene['OBJECT_TYPE'] == 'AGENT', 'X'] for scene in batch]\n",
        "    ys = [scene.loc[scene['OBJECT_TYPE'] == 'AGENT', 'Y'] for scene in batch]\n",
        "\n",
        "    # split X and Y values so that training has 45 datapoints and testing has 5 data points\n",
        "    input_xs = [scene[:45] for scene in xs]\n",
        "    input_ys = [scene[:45] for scene in ys]\n",
        "    output_xs = [scene[45:] for scene in xs]\n",
        "    output_ys = [scene[45:] for scene in ys]\n",
        "\n",
        "    # Input: batch_size x 2 x 45\n",
        "    inp = [np.dstack((x,y)).reshape(2, 45) for(x,y) in zip(input_xs,input_ys)]\n",
        "    # Output: batch size x 2 x 5\n",
        "    out = [np.dstack((x,y)).reshape(2, 5) for(x,y) in zip(output_xs,output_ys)]\n",
        "    \n",
        "    # Convert np.array into pytorch tensor\n",
        "    inp = torch.FloatTensor(inp)\n",
        "    out = torch.FloatTensor(out)\n",
        "    \n",
        "    return [inp, out]\n",
        "\n",
        "train_iter = DataLoader(train_data,batch_size=batch_sz, shuffle = False, collate_fn=my_collate, num_workers=0)\n",
        "train_in, train_label = next(iter(train_iter))\n",
        "train_in.size()"
      ],
      "metadata": {
        "id": "jmfid-y7eviw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}